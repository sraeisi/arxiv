{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#from nltk import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# nltk\n",
    "\n",
    "import nltk as nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "nltk_stopwords =  set(stopwords.words('english'))\n",
    "Morz_stopwords = list( pd.read_csv('Conditioner/longstopwords.txt',header=None )[0]     )\n",
    "# Collections\n",
    "from collections import Counter\n",
    "\n",
    "import pickle \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# sample_records = np.load('sample_abs.npz')\n",
    "sample_abs_list = np.load('sample_abs_list.npz')\n",
    "sample_abstract = sample_abs_list['abs_list'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "init_cell": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5552"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = pd.read_csv('Dictator/refinedDictionary.csv')\n",
    "dictionary = dictionary.set_index('0').to_dict()['1']\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'issue',\n",
       " 'is',\n",
       " 'the',\n",
       " 'expediency',\n",
       " 'of',\n",
       " 'the',\n",
       " 'QCD',\n",
       " 'notions',\n",
       " 'use',\n",
       " 'in',\n",
       " 'the',\n",
       " 'low',\n",
       " 'energy',\n",
       " 'region',\n",
       " 'ndown',\n",
       " 'to',\n",
       " 'the',\n",
       " 'confinement',\n",
       " 'scale',\n",
       " 'and',\n",
       " 'in',\n",
       " 'particular',\n",
       " 'the',\n",
       " 'efficacy',\n",
       " 'of',\n",
       " 'the',\n",
       " 'QCD',\n",
       " 'ninvariant',\n",
       " 'coupling',\n",
       " 'bar',\n",
       " 'alpha',\n",
       " '_s',\n",
       " 'Q',\n",
       " '2',\n",
       " 'with',\n",
       " 'a',\n",
       " 'minimal',\n",
       " 'analytic',\n",
       " 'modification',\n",
       " 'in',\n",
       " 'nthis',\n",
       " 'domain',\n",
       " 'To',\n",
       " 'this',\n",
       " 'goal',\n",
       " 'we',\n",
       " 'overview',\n",
       " 'a',\n",
       " 'quite',\n",
       " 'recent',\n",
       " 'progress',\n",
       " 'in',\n",
       " 'application',\n",
       " 'nof',\n",
       " 'the',\n",
       " 'ghost',\n",
       " 'free',\n",
       " 'Analytic',\n",
       " 'Perturbative',\n",
       " 'Theory',\n",
       " 'approach',\n",
       " 'with',\n",
       " 'no',\n",
       " 'adjustable',\n",
       " 'nparameters',\n",
       " 'for',\n",
       " 'QCD',\n",
       " 'in',\n",
       " 'the',\n",
       " 'region',\n",
       " 'below',\n",
       " '1',\n",
       " 'GeV',\n",
       " 'Among',\n",
       " 'them',\n",
       " 'the',\n",
       " 'Bethe',\n",
       " 'Salpeter',\n",
       " 'nanalysis',\n",
       " 'of',\n",
       " 'the',\n",
       " 'meson',\n",
       " 'spectra',\n",
       " 'and',\n",
       " 'spin',\n",
       " 'dependent',\n",
       " 'polarization',\n",
       " 'Bjorken',\n",
       " 'sum',\n",
       " 'nrule',\n",
       " 'The',\n",
       " 'impression',\n",
       " 'is',\n",
       " 'that',\n",
       " 'there',\n",
       " 'is',\n",
       " 'a',\n",
       " 'chance',\n",
       " 'for',\n",
       " 'the',\n",
       " 'theoretically',\n",
       " 'consistent',\n",
       " 'nand',\n",
       " 'numerically',\n",
       " 'correlated',\n",
       " 'description',\n",
       " 'of',\n",
       " 'hadronic',\n",
       " 'events',\n",
       " 'from',\n",
       " 'Z_0',\n",
       " 'till',\n",
       " 'a',\n",
       " 'few',\n",
       " 'nhundred',\n",
       " 'MeV',\n",
       " 'scale',\n",
       " 'by',\n",
       " 'combination',\n",
       " 'of',\n",
       " 'analytic',\n",
       " 'pQCD',\n",
       " 'and',\n",
       " 'some',\n",
       " 'explicit',\n",
       " 'nnon',\n",
       " 'perturbative',\n",
       " 'contribution',\n",
       " 'in',\n",
       " 'the',\n",
       " 'spirit',\n",
       " 'of',\n",
       " 'duality',\n",
       " 'This',\n",
       " 'is',\n",
       " 'an',\n",
       " 'invitation',\n",
       " 'nto',\n",
       " 'the',\n",
       " 'practitioner',\n",
       " 'community',\n",
       " 'for',\n",
       " 'a',\n",
       " 'more',\n",
       " 'courageous',\n",
       " 'use',\n",
       " 'of',\n",
       " 'ghost',\n",
       " 'free',\n",
       " 'QCD',\n",
       " 'ncoupling',\n",
       " 'models',\n",
       " 'for',\n",
       " 'data',\n",
       " 'analysis',\n",
       " 'in',\n",
       " 'the',\n",
       " 'low',\n",
       " 'energy',\n",
       " 'region',\n",
       " 'n',\n",
       " 'Comment',\n",
       " '12',\n",
       " 'pages',\n",
       " '5',\n",
       " 'figures',\n",
       " 'to',\n",
       " 'be',\n",
       " 'published',\n",
       " 'in',\n",
       " 'Quantum',\n",
       " 'Field',\n",
       " 'Theory',\n",
       " 'and',\n",
       " 'n',\n",
       " 'Beyond',\n",
       " 'Essays',\n",
       " 'in',\n",
       " 'honor',\n",
       " 'of',\n",
       " 'Wolfhart',\n",
       " 'Zimmermann',\n",
       " 'on',\n",
       " 'the',\n",
       " 'occasion',\n",
       " 'of',\n",
       " 'his',\n",
       " '80th',\n",
       " 'n',\n",
       " 'birthday',\n",
       " 'WS',\n",
       " 'Singapore']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(sample_abstract )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditionor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The objective of this function is to get an abstract and return \n",
    "a list of words in the abstract. \n",
    "The idea is to get rid of some non-neccessary words and turn \n",
    "everything into their roots, also get rid of the latex and numbers\n",
    "and ...\n",
    "This function is to be used for making the dictionary and also in \n",
    "the vectorizer. \n",
    "\"\"\"\n",
    "\n",
    "def abstract_tokenizor(abstract):\n",
    "#     abstract_tl =  word_tokenize(abstract) \n",
    "#     abstract_tl = [lemmatizer.lemmatize(i).lower() for i in word_tokenize(abstract) ]\n",
    "    abstract_tl = [lemmatizer.lemmatize(i).lower() for i in tokenizer.tokenize(abstract) ]\n",
    "    abstract_tl = [word for word in abstract_tl if \n",
    "                   (not word in nltk_stopwords) \n",
    "                   and (not word in Morz_stopwords)\n",
    "                  and (word.isalpha() )]\n",
    "    \"\"\"\n",
    "    1. Tokenize, => lowercase\n",
    "    2. Lemetize\n",
    "    3. get rid of stop words\n",
    "    4. get rid of latex and extra commands\n",
    "    \n",
    "    \"\"\"\n",
    "    return abstract_tl\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [i for i in  Counter(  abstract_tokenizor(sample_abstract) ) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Make a dictionary from all the abstract in the list\n",
    "'''\n",
    "\n",
    "def dictator(abs_list):\n",
    "    word_list = []\n",
    "    for i in abs_list:\n",
    "        word_list += abstract_tokenizor(i)\n",
    "        \n",
    "    return dict( Counter(word_list) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 890 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sample_dictionary = dictator( sample_abs_list['abs_list'][:500] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sample_dictionary).index('qcd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Takes an input abstract and returns the \n",
    "vector corresponding to that.\n",
    "'''\n",
    "def vectorizer(abstract, dictionary):\n",
    "    abstract_word_list = abstract_tokenizor(abstract)\n",
    "    dict_word_list = list(dictionary)\n",
    "    abst_dict = Counter(abstract_word_list)\n",
    "    word_count = len(abstract_word_list)\n",
    "    abstract_vec= [ abst_dict[key] / word_count for key in dict_word_list ]\n",
    "        \n",
    "    return abstract_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def vectorizer(abstract, dictionary):\n",
    "    abstract_word_list = abstract_tokenizor(abstract)\n",
    "    abstract_vec = np.zeros(len(dictionary))\n",
    "    dict_word_list = list(dictionary)\n",
    "    word_count = len(abstract_word_list)\n",
    "    for i in abstract_word_list:\n",
    "        if i in dict_word_list:\n",
    "            abstract_vec[dict_word_list.index(i) ] +=1\n",
    "        \n",
    "    return abstract_vec/sum(abstract_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(abstract, dictionary):\n",
    "#     Tokenize the abstract\n",
    "    abstract_word_list = abstract_tokenizor(abstract)\n",
    "#     Initiate abstract vector\n",
    "    abstract_vec = np.zeros(len(dictionary))\n",
    "#     Call the dictionary\n",
    "    dict_word_list = list(dictionary)\n",
    "    word_count = len(abstract_word_list)\n",
    "#     Abstract words Frequency\n",
    "    abst_dict = Counter(abstract_word_list)\n",
    "    \n",
    "    for i in abst_dict:\n",
    "        if i in dict_word_list:\n",
    "            abstract_vec[dict_word_list.index(i) ] += abst_dict[i]\n",
    "        \n",
    "    return abstract_vec/sum(abstract_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999996"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum( vectorizer(sample_abs_list['abs_list'][2] , dictionary) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7696,\n",
       " 7696,\n",
       " 7696,\n",
       " 7696,\n",
       " 7696,\n",
       " 7696,\n",
       " 7696,\n",
       " 7696,\n",
       " 7696,\n",
       " 7696,\n",
       " 7696,\n",
       " 7696,\n",
       " 7696,\n",
       " 7696,\n",
       " 7696,\n",
       " 7696,\n",
       " 7696,\n",
       " 7696,\n",
       " 7696,\n",
       " 7696,\n",
       " 7696,\n",
       " 7696,\n",
       " 7696,\n",
       " 7696,\n",
       " 7696,\n",
       " 7696,\n",
       " 7696,\n",
       " 7696,\n",
       " 7696,\n",
       " 7696]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "[len(vectorizer(sample_abs_list['abs_list'][i], sample_dictionary) ) for i in range(30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
